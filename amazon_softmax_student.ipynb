{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d255a80-cf9b-460a-ba1f-498cc632fc0f",
   "metadata": {},
   "source": [
    "## Student Model(with softmax output) training and evaluation on Amazon reviews polarity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8e92a3-db78-4ab0-ae77-bc397147e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ae2666-72c7-41ec-86fa-dd1c4a4e60b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from llm_classifier_modified import LLMClassifier\n",
    "from llm_model_modified1 import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874b21ca-81c8-4a9c-82de-d5f7b93e4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon reviews polarity train and test data\n",
    "df_train = pd.read_csv('train_amazon.csv', header=None)\n",
    "df_test = pd.read_csv('test_amazon.csv', header=None)\n",
    "\n",
    "n_train = 10000\n",
    "n_in_context = 5  \n",
    "n_total_in_context = len(df_train) * n_in_context  \n",
    "n_test = 5000\n",
    "n_val = 100\n",
    "\n",
    "df_train_actual = df_train.iloc[:n_train] \n",
    "df_in_context_base = df_train.iloc[n_train:n_train + n_total_in_context]\n",
    "df_val = df_train.iloc[n_train + n_total_in_context:n_train + n_total_in_context + n_val]\n",
    "df_test_actual = df_test.iloc[:n_test]  \n",
    "\n",
    "gt_labels_train = df_train_actual.iloc[:, 0].values.astype(int) \n",
    "samples_train = df_train_actual.iloc[:, 2].values \n",
    "gt_labels_val = df_val.iloc[:, 0].values.astype(int) \n",
    "samples_val = df_val.iloc[:, 2].values \n",
    "\n",
    "gt_labels_test = df_test_actual.iloc[:, 0].values.astype(int)\n",
    "samples_test = df_test_actual.iloc[:, 2].values  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2111c18a-1e30-4b96-b43e-ce5b20e05e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,110,656 || all params: 7,255,134,208 || trainable%: 0.0980\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt formatting class for sentiment classification and initializes an LLM-based classifier\n",
    "class PromptFormatting(object):\n",
    "    def __init__(self):\n",
    "        # Best instruction from BayesPE teacher i.e. instruction with highest weight\n",
    "        self.INSTRUCTION = 'classify the sentiment of the Amazon review below into one of the following classes:'\n",
    "        self.CLASSES = ['negative', 'positive']\n",
    "        self.CLASSES_FOR_MATCHING = [self.CLASSES, ['neg', 'pos'], ['1', '2']]\n",
    "        self.CLASSES_TEXT = '''1. {}\\n2. {}'''.format(self.CLASSES[0], self.CLASSES[1])\n",
    "\n",
    "    def format_instruction(self, instruction):\n",
    "        return '''{}\\n{}\\n'''.format(instruction, self.CLASSES_TEXT)\n",
    "\n",
    "    def format_content(self, content):\n",
    "        return '''review: {}\\nthe review is '''.format(content)\n",
    "\n",
    "llm = LLM(model_name=\"mistralai/Mistral-7B-Instruct-v0.3\", use_reduced_precision=True,use_lora=True)\n",
    "prompt_formatting = PromptFormatting()\n",
    "classifier = LLMClassifier(model=llm, prompt_formatting=prompt_formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64a5382-f643-4443-8df4-ad36279ecace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teacher predictions and weights\n",
    "probs = torch.load(\"amazon_probs.pt\", weights_only=False)\n",
    "weights = torch.load(\"amazon_prompt_weights.pt\", weights_only=False)\n",
    "if isinstance(probs, np.ndarray):\n",
    "    probs = torch.tensor(probs, dtype=torch.float32, device=llm.device)\n",
    "if isinstance(weights, np.ndarray):\n",
    "    weights = torch.tensor(weights, dtype=torch.float32, device=llm.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e1396a-bf7c-420c-992f-e8a8ef6ee676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute KL divergence loss\n",
    "def dirichlet_loss(student_probs, probs):\n",
    "    kl_loss = F.kl_div(student_probs.log(), probs, reduction='batchmean')\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3396e346-a531-48d0-80f4-b2e54cef9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DirichletDataset(Dataset):\n",
    "    def __init__(self, samples, num_samples):\n",
    "        self.samples = samples\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0161c507-a518-4714-aa86-aae058fb4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 14.995201678422745\n",
      "Epoch 2/10, Loss: 5.594062375283102\n",
      "Epoch 3/10, Loss: 3.152453308532131\n",
      "Epoch 4/10, Loss: 2.143950943296659\n",
      "Epoch 5/10, Loss: 2.2521223495932645\n",
      "Epoch 6/10, Loss: 2.2971361029849504\n",
      "Epoch 7/10, Loss: 1.934872161684325\n",
      "Epoch 8/10, Loss: 1.2925368519863696\n",
      "Epoch 9/10, Loss: 1.029416300902085\n",
      "Epoch 10/10, Loss: 0.947909543679998\n"
     ]
    }
   ],
   "source": [
    "# Train student model with teacher predictions\n",
    "def train_student(samples_train, probs, weights, num_epochs=10, learning_rate=1e-5, batch_size=32):\n",
    "    dataset = DirichletDataset(samples_train, len(samples_train))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, llm.model.parameters()), lr=learning_rate)\n",
    "    llm.model.train()  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (batch_samples, batch_indices) in enumerate(dataloader, start=1):\n",
    "            batch_indices = batch_indices.to(llm.device)\n",
    "\n",
    "            batch_probs = probs[batch_indices] \n",
    "            weights = weights.view(-1)\n",
    "\n",
    "                  \n",
    "            batch_probs = (batch_probs * weights) \n",
    "            batch_probs = batch_probs.sum(dim=2) \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            student_probs = classifier.soft_labels_batch(input_texts=batch_samples)\n",
    "            loss = dirichlet_loss(student_probs, batch_probs)\n",
    "          \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 1000 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(dataloader)}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss}\")\n",
    "\n",
    "train_student(samples_train, probs, weights, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a01a7c4-942c-4b88-9e5f-2c76a6cb2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of model on amazon reviews polarity test data\n",
    "class DirichletDataset(Dataset):\n",
    "    def __init__(self, samples, n_samples):\n",
    "        self.samples = samples\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "llm.model.eval()\n",
    "test_dataset = DirichletDataset(samples_test, n_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False) \n",
    "\n",
    "def get_test_alpha(test_dataloader, classifier):\n",
    "    all_alpha = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_samples in test_dataloader:\n",
    "            alpha_batch = classifier.soft_labels_batch(input_texts=batch_samples)\n",
    "            all_alpha.append(alpha_batch)\n",
    "\n",
    "    return torch.cat(all_alpha, dim=0) \n",
    "\n",
    "\n",
    "\n",
    "stu_probs = get_test_alpha(test_dataloader, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb6624c6-1024-467c-833f-dd81047e1b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student f1-score: 0.9589682890525113, Student ECE: 0.01881118305027485\n"
     ]
    }
   ],
   "source": [
    "import evaluation  \n",
    "stu_probs=stu_probs.cpu().numpy()\n",
    "f1_score = evaluation.compute_metric(gt_labels_test, stu_probs, metric='f1')\n",
    "ece = evaluation.compute_metric(gt_labels_test, stu_probs, metric='ece')\n",
    "print('Student f1-score: {}, Student ECE: {}'.format(f1_score, ece))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3788867-a7b2-4470-b8a0-81e6301de0b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluation on out-of-distribution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d79ba15b-d144-4a6e-a7d4-82cfe57013f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Yahoo Answers dataset\n",
    "df_train = pd.read_csv('train_yahoo.csv', header=None)\n",
    "df_test = pd.read_csv('test_yahoo.csv', header=None)\n",
    "\n",
    "n_train = 10000  \n",
    "n_in_context = 5  \n",
    "n_val = 100\n",
    "n_test = 5000\n",
    "\n",
    "# Split data\n",
    "df_train_actual = df_train.iloc[:n_train]\n",
    "df_test_actual = df_test.iloc[:n_test]\n",
    "\n",
    "# Format function for prompts\n",
    "def format_prompt(q1, q2, a):\n",
    "    return \"Question: \" + q1.astype(str) + \" \" + q2.astype(str) + \"\\nAnswer: \" + a.astype(str)\n",
    "\n",
    "# Extract training data\n",
    "gt_labels_train = df_train_actual.iloc[:, 0].values.astype(int)\n",
    "samples_train = format_prompt(df_train_actual.iloc[:, 1], df_train_actual.iloc[:, 2], df_train_actual.iloc[:, 3]).values\n",
    "\n",
    "gt_labels_test = df_test_actual.iloc[:, 0].values.astype(int)\n",
    "samples_test = format_prompt(df_test_actual.iloc[:, 1], df_test_actual.iloc[:, 2], df_test_actual.iloc[:, 3]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c427d52b-ecd5-4e89-83de-7dd9ffea4784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptFormatting(object):\n",
    "    def __init__(self):\n",
    "        self.INSTRUCTION = 'Identify the topic that the following question and answer belong to:'\n",
    "        self.CLASSES = [\n",
    "            ' ',\n",
    "    'Society & Culture',\n",
    "    'Science & Mathematics',\n",
    "    'Health',\n",
    "    'Education & Reference',\n",
    "    'Computers & Internet',\n",
    "    'Sports',\n",
    "    'Business & Finance',\n",
    "    'Entertainment & Music',\n",
    "    'Family & Relationships',\n",
    "    'Politics & Government'\n",
    "]\n",
    "        self.CLASSES_FOR_MATCHING = [self.CLASSES]\n",
    "        self.CLASSES_TEXT = '''1. {}\\n2. {}\\n3. {}\\n4. {}\\n5. {}\\n6. {}\\n7. {}\\n8. {}\\n9. {}\\n10. {}'''.format(self.CLASSES[1], self.CLASSES[2], self.CLASSES[3], self.CLASSES[4], self.CLASSES[5], self.CLASSES[6], self.CLASSES[7], self.CLASSES[8], self.CLASSES[9], self.CLASSES[10])\n",
    "\n",
    "    def format_instruction(self, instruction):\n",
    "        return '''{}\\n{}\\n'''.format(instruction, self.CLASSES_TEXT)\n",
    "\n",
    "    def format_content(self, content):\n",
    "        return '''{}\\nthe topic is '''.format(content)\n",
    "\n",
    "prompt_formatting = PromptFormatting()\n",
    "classifier = LLMClassifier(model=llm, prompt_formatting=prompt_formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a45aa17-a6b5-4a56-8cf8-e7627e86747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of model on Yahoo answers test data\n",
    "class DirichletDataset(Dataset):\n",
    "    def __init__(self, samples, n_samples):\n",
    "        self.samples = samples\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "llm.model.eval()\n",
    "test_dataset = DirichletDataset(samples_test, n_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False) \n",
    "\n",
    "def get_test_alpha(test_dataloader, classifier):\n",
    "    all_alpha = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_samples in test_dataloader:\n",
    "            alpha_batch = classifier.soft_labels_batch(input_texts=batch_samples)\n",
    "            all_alpha.append(alpha_batch)\n",
    "\n",
    "    return torch.cat(all_alpha, dim=0) \n",
    "\n",
    "\n",
    "\n",
    "stu_probs = get_test_alpha(test_dataloader, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa92cc47-323c-42f8-bdd8-dffa69613480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student f1-score: 0.5477921854622683, Student ECE: 0.23658406734466553\n"
     ]
    }
   ],
   "source": [
    "import evaluation  \n",
    "stu_probs=stu_probs.cpu().numpy()\n",
    "f1_score = evaluation.compute_metric(gt_labels_test, stu_probs, metric='f1')\n",
    "ece = evaluation.compute_metric(gt_labels_test, stu_probs, metric='ece')\n",
    "print('Student f1-score: {}, Student ECE: {}'.format(f1_score, ece))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff045ee5-fc98-4d37-8008-6f59acae58b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7640457  0.11008723 0.3987073  ... 1.3392062  0.31525946 0.01033301]\n"
     ]
    }
   ],
   "source": [
    "# Compute the predictive entropy on yahoo answers test data\n",
    "def entropy_numpy(probs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute entropy from probabilities for each row (example) in a NumPy array.\n",
    "    `probs`: shape [num_examples, num_classes]\n",
    "    Returns entropy: shape [num_examples]\n",
    "    \"\"\"\n",
    "    return entropy(stu_probs, axis=1)  # computes entropy along the class dimension\n",
    "    \n",
    "ent_yahoo = entropy_numpy(stu_probs)\n",
    "print(ent_yahoo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27b7e53e-8d2c-47e1-8995-73b3bb33f70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6054618\n"
     ]
    }
   ],
   "source": [
    "# Mean predictive entropy\n",
    "print(ent_yahoo.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df08f333-53b3-461b-b3d6-30aed6489a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sst2 dataset\n",
    "df_train = pd.read_csv('train_sst2.csv')\n",
    "df_test = pd.read_csv('test_sst2.csv')\n",
    "n_train = 10000  \n",
    "n_in_context = 5  \n",
    "n_total_in_context = 9 * n_in_context  \n",
    "n_val=100\n",
    "df_train_actual = df_train.iloc[:n_train] \n",
    "df_test_actual = df_test.iloc[:]  \n",
    "gt_labels_train = df_train_actual.iloc[:, 2].values.astype(int) \n",
    "samples_train = df_train_actual.iloc[:, 1].values \n",
    "gt_labels_test = df_test_actual.iloc[:, 2].values.astype(int)\n",
    "samples_test = df_test_actual.iloc[:, 1].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b8f48e6-3ce6-4ce3-8caa-f6da2815da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptFormatting(object):\n",
    "    def __init__(self):\n",
    "        self.INSTRUCTION = 'Select the sentiment category that best matches the opinion expressed in the review snippet.'\n",
    "        self.CLASSES = ['negative', 'positive']\n",
    "        self.CLASSES_FOR_MATCHING = [self.CLASSES, ['neg', 'pos'], ['1', '2']]\n",
    "        self.CLASSES_TEXT = '''1. {}\\n2. {}'''.format(self.CLASSES[0], self.CLASSES[1])\n",
    "\n",
    "    def format_instruction(self, instruction):\n",
    "        return '''{}\\n{}\\n'''.format(instruction, self.CLASSES_TEXT)\n",
    "\n",
    "    def format_content(self, content):\n",
    "        return '''review: {}\\nthe review is '''.format(content)\n",
    "\n",
    "prompt_formatting = PromptFormatting()\n",
    "classifier = LLMClassifier(model=llm, prompt_formatting=prompt_formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d31945c-8058-43f7-9788-51b4e89edaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of model on sst2 test data\n",
    "class DirichletDataset(Dataset):\n",
    "    def __init__(self, samples, n_samples):\n",
    "        self.samples = samples\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "llm.model.eval()\n",
    "test_dataset = DirichletDataset(samples_test, 872)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False) \n",
    "\n",
    "def get_test_alpha(test_dataloader, classifier):\n",
    "    all_alpha = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_samples in test_dataloader:\n",
    "            alpha_batch = classifier.soft_labels_batch(input_texts=batch_samples)\n",
    "            all_alpha.append(alpha_batch)\n",
    "\n",
    "    return torch.cat(all_alpha, dim=0) \n",
    "\n",
    "stu_probs = get_test_alpha(test_dataloader, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fafb3b6-b4cc-4d95-ae44-1a71ab5b675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student f1-score: 0.9483830232236934, Student ECE: 0.030479537323117256\n"
     ]
    }
   ],
   "source": [
    "import evaluation  \n",
    "stu_probs=stu_probs.cpu().numpy()\n",
    "f1_score = evaluation.compute_metric(gt_labels_test, stu_probs, metric='f1')\n",
    "ece = evaluation.compute_metric(gt_labels_test, stu_probs, metric='ece')\n",
    "print('Student f1-score: {}, Student ECE: {}'.format(f1_score, ece))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fae71f02-4fb1-4c5a-b2d4-07ba2f28ea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.50894838e-04 3.96795012e-03 8.55409773e-04 1.11426064e-03\n",
      " 1.85965863e-03 4.10740450e-03 6.62713200e-02 5.76008439e-01\n",
      " 8.41927191e-04 4.35562851e-03 9.44217958e-04 4.40091500e-03\n",
      " 2.62905564e-02 4.57564890e-01 1.94608793e-03 8.31549056e-04\n",
      " 5.71298599e-01 5.27614611e-04 9.60300211e-03 3.16841295e-03\n",
      " 3.10213596e-01 1.01728719e-02 1.39706712e-02 6.49171066e-04\n",
      " 1.15419098e-03 1.43031366e-02 4.69876640e-03 4.89693973e-03\n",
      " 2.43290840e-03 1.92579790e-03 1.72192173e-03 3.31451744e-03\n",
      " 8.04147101e-04 4.00499851e-02 3.00773047e-03 4.10983711e-02\n",
      " 1.58589741e-03 2.85308272e-01 2.30503222e-03 9.13104101e-04\n",
      " 7.43988960e-04 1.34720095e-03 1.60268337e-01 8.75237340e-04\n",
      " 2.29302887e-03 4.61061954e-01 3.36075597e-03 3.49129667e-03\n",
      " 7.58618349e-04 2.54554977e-03 2.51035555e-03 1.98733737e-03\n",
      " 2.40848988e-01 1.27145126e-02 1.80523982e-03 1.00607274e-03\n",
      " 9.15724318e-03 2.39584267e-01 4.55524167e-03 2.42446247e-03\n",
      " 1.43257820e-03 1.83217116e-02 5.42911608e-03 1.22100220e-03\n",
      " 2.01691031e-01 1.33282086e-02 5.23517370e-01 5.30445366e-04\n",
      " 1.50465663e-03 4.12160205e-03 1.50883123e-01 1.18922722e-03\n",
      " 1.44011562e-03 7.09967166e-02 3.50341061e-03 5.63815469e-03\n",
      " 3.40765622e-03 4.41613747e-03 7.69829080e-02 1.46562839e-03\n",
      " 4.88016335e-03 3.46717448e-03 4.40091500e-03 1.81154232e-03\n",
      " 1.40028854e-03 2.89187767e-02 3.16841295e-03 8.32544453e-03\n",
      " 6.37381256e-01 2.42446247e-03 3.50645259e-02 1.37378359e-02\n",
      " 6.92872643e-01 8.13962519e-02 1.63959409e-03 6.98960871e-02\n",
      " 2.97141075e-03 7.79004476e-04 1.80209568e-03 2.12789848e-02\n",
      " 9.29398113e-04 3.41946678e-03 6.60986304e-01 7.27059669e-04\n",
      " 2.00445130e-02 6.12190599e-03 5.61397232e-04 2.27710721e-03\n",
      " 1.98039110e-03 1.75845018e-03 5.10341115e-03 5.31833200e-03\n",
      " 3.95425456e-03 5.63815469e-03 1.12600140e-02 6.08010963e-03\n",
      " 1.11624622e-03 6.00140929e-01 1.22516882e-02 5.69717169e-01\n",
      " 1.40270882e-03 7.64552783e-03 1.99081842e-03 1.16645684e-03\n",
      " 5.91561664e-03 6.96884003e-04 3.01819295e-03 8.01933277e-03\n",
      " 9.26119334e-04 1.79576199e-03 7.33872969e-03 1.23345759e-02\n",
      " 2.47995928e-03 1.12220217e-02 1.19343004e-03 1.00699486e-02\n",
      " 9.25105158e-03 1.45535334e-03 2.55447277e-03 1.04260817e-01\n",
      " 1.22516882e-02 7.46508688e-03 2.58576241e-03 2.30254591e-01\n",
      " 3.84645606e-03 3.58932046e-03 1.08487923e-02 8.29182342e-02\n",
      " 8.35386198e-03 2.13481206e-03 2.35778885e-03 1.02762040e-03\n",
      " 1.65755339e-02 2.73850164e-03 6.87753083e-03 3.56457056e-03\n",
      " 2.81570922e-03 3.08173569e-03 1.54401958e-01 2.63772398e-01\n",
      " 1.37378359e-02 4.45451401e-02 5.89529937e-03 5.08586969e-03\n",
      " 3.42748240e-02 1.38770835e-02 6.92773461e-01 1.77496731e-01\n",
      " 1.33080315e-03 8.33041850e-04 1.32376084e-03 6.92659140e-01\n",
      " 1.18091924e-03 6.20983839e-01 2.25340133e-03 2.80772354e-02\n",
      " 6.31362060e-03 3.36383134e-01 2.24162708e-03 8.66046001e-04\n",
      " 2.84513412e-03 1.75845018e-03 1.70816705e-02 1.47006333e-01\n",
      " 2.57227919e-03 6.53786876e-04 2.35778885e-03 1.40648726e-02\n",
      " 2.21829419e-03 4.78039449e-03 1.48718715e-01 7.13079877e-04\n",
      " 2.89187767e-02 3.46717448e-03 6.60093129e-03 1.42799169e-01\n",
      " 1.02076992e-01 9.86719504e-03 5.83726563e-04 1.76027678e-02\n",
      " 1.01460665e-01 1.61918849e-02 3.34333861e-03 8.91165342e-03\n",
      " 5.77524537e-03 4.88016335e-03 2.69127940e-03 1.61392381e-03\n",
      " 9.87283960e-02 2.92535988e-03 9.39294812e-04 5.38714111e-01\n",
      " 1.70993037e-03 7.93772936e-03 8.31743926e-02 5.26379282e-03\n",
      " 8.35974759e-04 4.92494702e-01 5.55242240e-01 1.75230624e-03\n",
      " 7.18992297e-03 1.08487923e-02 8.46400566e-04 1.12743527e-01\n",
      " 4.65932637e-02 2.32925965e-03 2.04481073e-02 5.96308615e-04\n",
      " 3.76743451e-03 4.40091500e-03 1.38305053e-02 1.67432602e-03\n",
      " 5.78695051e-02 1.46924462e-02 2.03127116e-02 8.03958848e-02\n",
      " 1.35087743e-02 3.66461743e-03 1.27369672e-01 1.61112589e-03\n",
      " 1.00080681e-03 8.03958848e-02 4.76395106e-03 1.43031366e-02\n",
      " 8.27143725e-04 1.75537600e-03 1.06629252e-03 2.23771366e-03\n",
      " 3.23214568e-02 5.33678591e-01 7.09281419e-04 4.34063841e-03\n",
      " 1.72192173e-03 7.38902576e-03 1.12216163e-03 3.92704085e-03\n",
      " 4.03708499e-03 4.58679767e-03 3.88654415e-03 1.60548324e-03\n",
      " 5.64939141e-01 8.41093250e-03 3.13562644e-03 1.19132712e-03\n",
      " 2.35013627e-02 1.30177885e-02 5.68129957e-01 1.52957933e-02\n",
      " 2.04365747e-03 3.53996409e-03 2.97658495e-03 4.83000558e-03\n",
      " 2.77675968e-03 5.45375586e-01 1.08122034e-02 6.54945383e-04\n",
      " 2.05843728e-02 1.06820499e-03 3.58715206e-02 4.84193005e-02\n",
      " 7.99201708e-03 1.16468938e-02 9.93437227e-03 1.87709734e-01\n",
      " 3.74147692e-03 1.74273998e-02 5.10341115e-03 1.16645684e-03\n",
      " 6.72599126e-04 1.59810528e-01 5.29480458e-04 2.76199058e-02\n",
      " 8.36889520e-02 2.16855574e-03 5.69439493e-04 1.87709734e-01\n",
      " 6.44465303e-03 6.20983839e-01 1.09225037e-02 2.16855574e-03\n",
      " 8.21341295e-04 9.66494530e-02 4.26325278e-04 1.98039110e-03\n",
      " 1.32843247e-03 1.46431848e-02 1.00965658e-03 3.78047023e-03\n",
      " 6.75873280e-01 1.14007073e-03 5.55326007e-02 4.08468361e-04\n",
      " 3.33242327e-01 4.75966284e-04 1.59985782e-03 2.10151682e-03\n",
      " 5.45728137e-04 6.24901662e-03 5.30006178e-03 4.50839475e-03\n",
      " 1.62809587e-03 5.76008439e-01 2.30102520e-03 4.55582663e-02\n",
      " 6.81676149e-01 4.88863140e-02 3.22387903e-03 2.39092857e-03\n",
      " 2.79851556e-02 1.63959409e-03 7.60326385e-02 7.51620764e-03\n",
      " 1.14906169e-02 5.41388728e-02 8.89303978e-04 1.16846245e-03\n",
      " 1.19969877e-03 2.53671245e-03 5.97673282e-03 1.64243462e-03\n",
      " 3.31451744e-03 1.55834854e-03 5.93587151e-03 3.94065771e-03\n",
      " 3.31451744e-03 1.12016697e-03 1.58314162e-03 6.98960871e-02\n",
      " 2.09417171e-03 1.26717333e-02 3.23503441e-03 2.68535197e-01\n",
      " 1.95293140e-03 4.90534003e-04 1.24600092e-02 4.83000558e-03\n",
      " 8.67251586e-03 4.00927570e-03 1.53074265e-01 1.30533404e-03\n",
      " 1.28937815e-03 5.50422538e-03 6.08010963e-03 4.10740450e-03\n",
      " 1.97346578e-03 3.30301211e-03 6.16666279e-04 7.34748095e-02\n",
      " 1.93932420e-03 2.11621379e-03 6.08010963e-03 5.69649041e-03\n",
      " 5.26713615e-04 2.59025046e-03 2.14599003e-03 4.50575233e-01\n",
      " 5.06843720e-03 6.33746982e-01 2.27710721e-03 1.66321218e-01\n",
      " 3.09241493e-03 1.36389327e-03 5.45375586e-01 7.68025464e-04\n",
      " 1.13747315e-02 6.60986304e-01 1.73702731e-03 3.41946678e-03\n",
      " 6.80484772e-01 3.94065771e-03 3.44325090e-03 2.29636487e-02\n",
      " 4.15018061e-03 1.07763149e-03 4.61853435e-03 1.47590600e-03\n",
      " 3.83822073e-04 1.15419098e-03 2.42865272e-03 6.60037100e-01\n",
      " 6.94379851e-04 6.48892298e-03 1.64649934e-02 3.95425456e-03\n",
      " 8.55409773e-04 6.37381256e-01 1.27359061e-03 7.85697903e-03\n",
      " 8.10167566e-03 1.32843247e-03 4.10740450e-03 3.05747688e-02\n",
      " 1.03306933e-03 5.26379282e-03 7.63972930e-04 2.10151682e-03\n",
      " 1.62246171e-03 1.25802902e-03 2.92535988e-03 2.93005146e-02\n",
      " 4.38573631e-03 4.74755513e-03 6.82250500e-01 2.28421688e-01\n",
      " 8.76142178e-03 1.12981107e-02 3.84645606e-03 1.05322385e-03\n",
      " 1.13603007e-03 8.75237340e-04 5.13775507e-04 5.44841625e-02\n",
      " 7.02004321e-03 7.43988960e-04 1.21283336e-02 2.09787139e-03\n",
      " 5.79118252e-01 2.24162708e-03 6.76226919e-04 6.49171066e-04\n",
      " 2.60321796e-02 1.76027678e-02 2.84513412e-03 6.84617902e-04\n",
      " 5.56123909e-03 1.40519219e-03 8.38235766e-03 6.20983839e-01\n",
      " 6.92773461e-01 4.32738326e-02 9.42614104e-04 4.40961448e-04\n",
      " 1.04035356e-03 1.04404229e-03 1.65201612e-02 1.63959409e-03\n",
      " 4.28111060e-03 1.22745661e-03 1.69679150e-02 1.66855333e-03\n",
      " 4.55524167e-03 4.82646115e-02 2.06528828e-02 2.78642448e-03\n",
      " 4.00499851e-02 1.52445920e-02 3.37243010e-03 1.24180866e-02\n",
      " 8.41927191e-04 4.02319012e-03 9.85034625e-04 2.65408074e-03\n",
      " 6.87616110e-01 9.32686904e-04 6.42267009e-03 1.70393183e-03\n",
      " 8.82121827e-03 1.64438695e-01 3.57986063e-01 1.48630550e-03\n",
      " 9.90287866e-04 5.38714111e-01 1.37111836e-03 2.54424214e-01\n",
      " 3.85249220e-02 4.34063841e-03 1.39781262e-03 4.67429087e-02\n",
      " 5.61397232e-04 2.30904529e-03 2.39092857e-03 2.32515717e-03\n",
      " 1.05695205e-03 1.88587734e-03 8.29716586e-03 1.52445920e-02\n",
      " 4.15018061e-03 2.78642448e-03 1.79889682e-03 1.42005179e-03\n",
      " 7.96483457e-03 4.85745072e-02 1.87923363e-03 2.31708935e-03\n",
      " 3.80393554e-04 3.46111543e-02 8.50894838e-04 3.19049018e-03\n",
      " 1.14406424e-03 1.38695985e-01 7.09967166e-02 1.51528988e-03\n",
      " 1.33731561e-02 9.69439819e-02 2.73489840e-02 1.34181632e-02\n",
      " 4.25825380e-02 6.83887839e-01 2.71948171e-03 1.38770835e-02\n",
      " 7.89165050e-02 2.44993460e-03 5.58037171e-03 1.04404229e-03\n",
      " 2.13481206e-03 7.00565230e-04 9.12048221e-02 1.07029723e-02\n",
      " 9.63556468e-02 3.12476140e-03 1.36389327e-03 6.94603324e-02\n",
      " 3.68621588e-01 1.77389046e-03 1.07763149e-03 3.03394487e-03\n",
      " 1.02768186e-02 2.81570922e-03 1.78944715e-03 9.86719504e-03\n",
      " 5.50328732e-01 1.25582784e-03 7.28877168e-03 2.89499504e-03\n",
      " 7.41643384e-02 6.24901662e-03 3.70552428e-02 5.30304253e-01\n",
      " 4.29960638e-02 2.29302887e-03 6.16399758e-03 4.88016335e-03\n",
      " 5.39193861e-03 2.90512643e-03 2.02585361e-03 2.98176706e-03\n",
      " 3.20154778e-03 1.09676190e-03 3.63695025e-01 3.06749828e-02\n",
      " 1.57758326e-03 5.34543693e-02 2.30904529e-03 2.15348951e-03\n",
      " 1.84025860e-03 1.70393183e-03 2.37355120e-02 2.05794885e-03\n",
      " 2.18377169e-03 6.44643267e-04 7.18992297e-03 4.65932637e-02\n",
      " 3.24628106e-03 1.19343004e-03 2.25340133e-03 2.39718985e-02\n",
      " 1.19132712e-03 2.24554585e-03 4.34133708e-02 4.66651144e-03\n",
      " 1.92579790e-03 1.24699727e-03 3.81992315e-03 1.66867115e-02\n",
      " 5.97673282e-03 3.28022055e-03 6.05927035e-03 2.81570922e-03\n",
      " 2.71007279e-03 7.26395659e-03 1.98733737e-03 1.46048306e-03\n",
      " 3.94065771e-03 1.13603007e-03 7.40047486e-04 1.72536857e-02\n",
      " 2.99801789e-02 2.24377699e-02 1.49676809e-03 3.43137141e-03\n",
      " 2.85505992e-03 6.29200554e-03 1.12981107e-02 6.25520421e-04\n",
      " 1.05846278e-01 7.88633130e-04 1.42005179e-03 6.84404790e-01\n",
      " 2.20673555e-03 1.55021893e-02 1.98039110e-03 7.68025464e-04\n",
      " 3.87314265e-03 5.77582454e-04 4.14141715e-01 1.27574075e-02\n",
      " 3.44325090e-03 5.89529937e-03 1.76501006e-01 3.49129667e-03\n",
      " 4.10740450e-03 9.06438101e-03 8.87665257e-04 1.71388593e-02\n",
      " 8.21285322e-03 2.16101157e-03 5.57472929e-04 3.46717448e-03\n",
      " 4.89690807e-04 6.12190599e-03 2.07966543e-03 5.22760674e-02\n",
      " 2.99729826e-03 1.22961472e-03 1.46431848e-02 1.51756346e-01\n",
      " 3.03921057e-03 8.16481262e-02 6.78407820e-03 9.76729114e-03\n",
      " 1.45279407e-03 1.70393183e-03 2.10151682e-03 3.85972066e-03\n",
      " 1.32612465e-03 3.87751982e-02 4.59313273e-01 1.26717333e-02\n",
      " 6.93139553e-01 1.39046670e-03 2.46706186e-03 5.37345838e-03\n",
      " 3.50341061e-03 3.60177900e-03 2.04365747e-03 1.75441429e-02\n",
      " 1.96486395e-02 3.36075597e-03 1.38560764e-03 7.02004321e-03\n",
      " 1.03121053e-03 3.58715206e-02 4.38573631e-03 7.04399776e-03\n",
      " 2.74799438e-03 1.25020454e-02 4.83000558e-03 3.04236174e-01\n",
      " 1.57483993e-03 2.92046275e-02 1.21283336e-02 8.37474305e-04\n",
      " 6.27486944e-01 9.47355572e-03 1.28034491e-03 1.77996337e-01\n",
      " 1.34253222e-03 1.90239400e-03 1.99430413e-03 1.61112589e-03\n",
      " 2.77107935e-02 4.71496256e-03 2.74799438e-03 5.69717169e-01\n",
      " 9.26099196e-02 5.20977611e-03 1.20605598e-03 1.87923363e-03\n",
      " 1.95833948e-02 3.30301211e-03 3.74147692e-03 4.38369870e-01\n",
      " 7.84290060e-02 3.56457056e-03 2.28421688e-01 5.37345838e-03\n",
      " 2.45845621e-03 5.16138002e-02 1.52445920e-02 1.24023080e-01\n",
      " 3.91352270e-03 4.43140371e-03 1.79889682e-03 5.89529937e-03\n",
      " 4.25166171e-03 4.15018061e-03 7.39338398e-02 5.25218666e-01\n",
      " 6.11306808e-04 8.61371215e-03 3.53934497e-01 1.82103191e-03\n",
      " 3.19049018e-03 2.64641680e-02 5.37979533e-04 1.71594659e-03\n",
      " 2.62657180e-03 2.31708935e-03 1.00431475e-03 8.94198474e-03\n",
      " 1.57483993e-03 2.96624424e-03 4.81103919e-02 8.37474305e-04\n",
      " 9.28249303e-03 9.62744467e-04 1.91909447e-03 1.55015849e-03\n",
      " 9.43220407e-02 1.80209568e-03 1.69204839e-03 1.02696732e-01\n",
      " 2.28903955e-03 4.88016335e-03 1.19657312e-02 1.24180866e-02\n",
      " 2.52451956e-01 1.03487133e-03 1.17053068e-03 2.29302887e-03\n",
      " 1.33311795e-03 6.62847281e-01 2.63772272e-02 3.45516531e-03\n",
      " 5.75040765e-02 1.28482492e-03 1.32146024e-03 2.07238505e-03\n",
      " 9.66843683e-03 2.24377699e-02 7.41430884e-03 1.99117027e-02\n",
      " 6.01588190e-01 2.10151682e-03 5.65750524e-03 9.74719180e-04\n",
      " 6.80731377e-03 2.19901814e-03 5.77582454e-04 2.79617729e-03\n",
      " 6.03027582e-01 1.33781810e-03 5.88930794e-04 1.33282086e-02\n",
      " 9.80050117e-03 2.06155583e-01 1.66561408e-03 4.91438026e-04\n",
      " 7.51620764e-03 8.27143725e-04 2.61743646e-03 5.58037171e-03\n",
      " 3.71756777e-02 1.49911176e-02 4.60261339e-03 1.09739870e-01\n",
      " 1.64533919e-03 6.98960871e-02 6.32233161e-04 4.00927570e-03\n",
      " 5.16684771e-01 3.43137141e-03 5.16684771e-01 1.74618093e-03\n",
      " 8.41093250e-03 1.38076325e-03 5.50422538e-03 4.19332879e-03\n",
      " 7.03028811e-04 2.34962418e-03 5.08586969e-03 1.14518842e-02\n",
      " 1.56064378e-02 5.84216826e-02 1.81786402e-03 1.31680979e-03\n",
      " 3.38312015e-02 4.10740450e-03 9.96803865e-03 5.51573117e-04\n",
      " 2.39927485e-03 1.14786401e-01 1.33282086e-02 2.63567478e-03\n",
      " 1.33731561e-02 6.64658010e-01 2.05794885e-03 5.35172934e-04\n",
      " 3.85915919e-04 1.17260218e-03 3.75058560e-04 1.68548636e-02\n",
      " 3.02760974e-02 2.90005701e-03 3.10312584e-03 5.04688185e-04\n",
      " 3.07102804e-03 1.43510662e-03 3.06035113e-03 6.47707105e-01\n",
      " 1.20605598e-03 1.43510662e-03 2.72897771e-03 1.91926196e-01\n",
      " 2.21424848e-02 7.00565230e-04 8.37474305e-04 1.61112589e-03\n",
      " 2.68147402e-02 6.27051061e-03 1.55542456e-02 5.93587151e-03\n",
      " 1.05590792e-02 3.02868662e-03 2.54554977e-03 5.75507351e-04\n",
      " 6.05927035e-03 1.74273998e-02 2.76199058e-02 3.66461743e-03\n",
      " 2.36193952e-03 6.94839936e-03 1.37083545e-01 5.16138002e-02\n",
      " 1.38076325e-03 2.66332459e-03 9.96316820e-02 5.21813333e-01\n",
      " 1.10646698e-03 1.70816705e-02 1.98456105e-02 1.06807917e-01\n",
      " 1.56636387e-01 9.20455828e-02 3.30681056e-02 1.41760544e-03\n",
      " 1.12069778e-01 9.72949609e-04 1.58865715e-03 1.42256159e-03\n",
      " 1.14131970e-02 2.16855574e-03 2.64641680e-02 1.60268337e-01\n",
      " 1.48630550e-03 6.81087554e-01 4.89693973e-03 3.81992315e-03\n",
      " 2.77675968e-03 7.21447216e-03 3.06035113e-03 2.64486438e-03]\n"
     ]
    }
   ],
   "source": [
    "# Predictive entropy on sst2 test data\n",
    "ent_sst2 = entropy_numpy(stu_probs)\n",
    "print(ent_sst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "badb19d7-1067-4875-a5b0-06d2e45310c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059649516\n"
     ]
    }
   ],
   "source": [
    "print(ent_sst2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5d28d-cc5e-4e6b-aa3c-193d632fb060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1ed6801-cd7f-41a5-8145-01de4eadcd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load youtube comments dataset\n",
    "df_train = pd.read_csv('youtube.csv')\n",
    "n_train = 1100  \n",
    "n_in_context = 5 \n",
    "\n",
    "n_total_in_context = 9 * n_in_context  \n",
    "n_val=100\n",
    "df_train_actual = df_train.iloc[:n_train] \n",
    "df_in_context_base = df_train.iloc[n_train:n_train + n_total_in_context]\n",
    "df_val = df_train.iloc[n_train + n_total_in_context:n_train+n_total_in_context+n_val]\n",
    "df_test_actual = df_train.iloc[n_train+n_total_in_context+n_val:]  \n",
    "\n",
    "gt_labels_train = df_train_actual.iloc[:, 4].values.astype(int) \n",
    "samples_train = df_train_actual.iloc[:, 3].values \n",
    "gt_labels_val = df_val.iloc[:, 4].values.astype(int) \n",
    "samples_val = df_val.iloc[:, 3].values \n",
    "gt_labels_test = df_test_actual.iloc[:, 4].values.astype(int)\n",
    "samples_test = df_test_actual.iloc[:, 3].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0024d1c-5f9d-4186-8028-4ecf578b88c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptFormatting(object):\n",
    "    def __init__(self):\n",
    "        self.INSTRUCTION = 'Judge whether the Youtube comment should be flagged as spam.'\n",
    "        self.CLASSES = ['not spam', 'spam']\n",
    "        self.CLASSES_FOR_MATCHING = [self.CLASSES, ['ham', 'spam'], ['0', '1']]\n",
    "        self.CLASSES_TEXT = '''1. {}\\n2. {}'''.format(self.CLASSES[0], self.CLASSES[1])\n",
    "\n",
    "    def format_instruction(self, instruction):\n",
    "        return '''{}\\n{}\\n'''.format(instruction, self.CLASSES_TEXT)\n",
    "\n",
    "    def format_content(self, content):\n",
    "        return '''comment: {}\\nthe comment is '''.format(content)\n",
    "\n",
    "prompt_formatting = PromptFormatting()\n",
    "classifier = LLMClassifier(model=llm, prompt_formatting=prompt_formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "671650d5-d376-4043-818d-6e78c36e8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of model on youtube comments test data\n",
    "class DirichletDataset(Dataset):\n",
    "    def __init__(self, samples, n_samples):\n",
    "        self.samples = samples\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "llm.model.eval()\n",
    "test_dataset = DirichletDataset(samples_test, 711)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False) \n",
    "\n",
    "def get_test_alpha(test_dataloader, classifier):\n",
    "    all_alpha = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_samples in test_dataloader:\n",
    "            alpha_batch = classifier.soft_labels_batch(input_texts=batch_samples)\n",
    "            all_alpha.append(alpha_batch)\n",
    "\n",
    "    return torch.cat(all_alpha, dim=0) \n",
    "\n",
    "\n",
    "\n",
    "stu_probs = get_test_alpha(test_dataloader, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3e667a2-dd1a-46ae-bb8b-90fc512346c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student f1-score: 0.6526290734770916, Student ECE: 0.15583941340446472\n"
     ]
    }
   ],
   "source": [
    "import evaluation  \n",
    "stu_probs=stu_probs.cpu().numpy()\n",
    "f1_score = evaluation.compute_metric(gt_labels_test, stu_probs, metric='f1')\n",
    "ece = evaluation.compute_metric(gt_labels_test, stu_probs, metric='ece')\n",
    "print('Student f1-score: {}, Student ECE: {}'.format(f1_score, ece))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d647df40-3945-4089-b8a9-2527ef1a9bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68108755 0.4733035  0.65510744 0.4924947  0.4924947  0.14531085\n",
      " 0.0855129  0.18770973 0.598686   0.598686   0.28673667 0.5370392\n",
      " 0.3835734  0.6804848  0.5336786  0.6087053  0.69040054 0.598686\n",
      " 0.23086795 0.42448777 0.6561176  0.6833564  0.31778398 0.10269673\n",
      " 0.2968649  0.5633357  0.27128315 0.52521867 0.31778398 0.31322876\n",
      " 0.10054242 0.10176843 0.69265914 0.23395337 0.29979986 0.5286122\n",
      " 0.65510744 0.36533386 0.19139497 0.36369503 0.4262181  0.30127415\n",
      " 0.4418527  0.09092619 0.38861087 0.2838846  0.4021613  0.30127415\n",
      " 0.17799634 0.67138517 0.01731138 0.07965304 0.6822505  0.41586155\n",
      " 0.37856194 0.5387141  0.5166848  0.4924947  0.4523219  0.29249662\n",
      " 0.33167794 0.54703087 0.16490765 0.27683643 0.52181333 0.1983931\n",
      " 0.41758323 0.05518102 0.46630877 0.50117457 0.43663013 0.67859197\n",
      " 0.11686215 0.4942335  0.54703087 0.52010643 0.10908221 0.18562979\n",
      " 0.12111352 0.17550981 0.3208422  0.21415447 0.65305144 0.3254612\n",
      " 0.6765737  0.6432272  0.0948991  0.633747   0.6804848  0.0683818\n",
      " 0.5957534  0.6672783  0.526917   0.16490765 0.36369503 0.17354122\n",
      " 0.5166848  0.4898842  0.31931096 0.53199303 0.61287665 0.56172705\n",
      " 0.30127415 0.02038035 0.08604069 0.4523219  0.01603013 0.2685352\n",
      " 0.14871871 0.43836987 0.6867718  0.69068134 0.6114947  0.6736901\n",
      " 0.6183173  0.3835734  0.08525017 0.6849073  0.37856194 0.43836987\n",
      " 0.26444918 0.27128315 0.68840146 0.60729814 0.6262054  0.6930251\n",
      " 0.58678174 0.4680577  0.18770973 0.49597102 0.30127415 0.05570914\n",
      " 0.39706093 0.35555243 0.48727107 0.6581017  0.30127415 0.31931096\n",
      " 0.4907546  0.67138517 0.11686215 0.2617492  0.39706093 0.6236135\n",
      " 0.31171903 0.069027   0.431418   0.53199303 0.5080887  0.6765737\n",
      " 0.3395398  0.6325151  0.57911825 0.3087126  0.42448777 0.31931096\n",
      " 0.5166848  0.42103207 0.30127415 0.18051177 0.29540426 0.40386623\n",
      " 0.69143355 0.69143355 0.53199303 0.18562979 0.39029557 0.6609863\n",
      " 0.10616592 0.598686   0.44882923 0.6822505  0.4227591  0.10745336\n",
      " 0.52521867 0.53199303 0.53199303 0.67859197 0.64548945 0.30127415\n",
      " 0.3686216  0.43663013 0.68877184 0.6571157  0.2881697  0.49770704\n",
      " 0.50981236 0.33559638 0.6628473  0.3752359  0.14743286 0.40386623\n",
      " 0.45581684 0.4785462  0.5852622  0.31931096 0.6432272  0.6169704\n",
      " 0.6432272  0.548682   0.36042753 0.23457412 0.36533386 0.64987946\n",
      " 0.30127415 0.5235174  0.52010643 0.6798679  0.47155517 0.37192234\n",
      " 0.13950841 0.69311666 0.19299209 0.14871871 0.01926032 0.0521098\n",
      " 0.36369503 0.6929927  0.6540854  0.67923695 0.50463533 0.6443639\n",
      " 0.6030276  0.21415447 0.15529245 0.6863276  0.10908221 0.6736901\n",
      " 0.664658   0.30572388 0.5972234  0.52181333 0.04070215 0.31778398\n",
      " 0.4785462  0.6432272  0.12328999 0.10777735 0.2617492  0.3254612\n",
      " 0.5633357  0.2604065  0.06523899 0.61965513 0.187188   0.20559326\n",
      " 0.6909472  0.5852622  0.68946815 0.68720126 0.4855278  0.14871871\n",
      " 0.07393384 0.24275538 0.34271204 0.36533386 0.43663013 0.6466039\n",
      " 0.55032873 0.23832437 0.18511276 0.2968649  0.53536063 0.05805301\n",
      " 0.68877184 0.22001372 0.67216694 0.6918595  0.58980036 0.30127415\n",
      " 0.6477071  0.6628473  0.18254496 0.14871871 0.19459987 0.65510744\n",
      " 0.6916541  0.54371613 0.48901343 0.6849073  0.24595705 0.14871871\n",
      " 0.22659981 0.6876161  0.22239158 0.27964154 0.63496876 0.65510744\n",
      " 0.10426082 0.6001409  0.52010643 0.3507095  0.0521098  0.6740625\n",
      " 0.36042753 0.08446624 0.57911825 0.6863276  0.2370694  0.12147392\n",
      " 0.39198303 0.506363   0.04355334 0.36533386 0.34271204 0.6581017\n",
      " 0.553609   0.05991849 0.43836987 0.20224488 0.4924947  0.2472463\n",
      " 0.4785462  0.6916541  0.6561176  0.4785462  0.6758733  0.31626126\n",
      " 0.5806639  0.06989609 0.23086795 0.30572388 0.08899698 0.4820385\n",
      " 0.62230337 0.64879894 0.1526339  0.2604065  0.6287588  0.5080887\n",
      " 0.69143355 0.30423617 0.6236135  0.6916541  0.5132532  0.28246552\n",
      " 0.5132532  0.6880162  0.07626919 0.2095546  0.58980036 0.08291823\n",
      " 0.6015882  0.6015882  0.67923695 0.42448777 0.3523202  0.43924016\n",
      " 0.6849073  0.598686   0.25906855 0.6287588  0.6001409  0.02988217\n",
      " 0.36205956 0.68912745 0.05177861 0.28673667 0.14871871 0.06668852\n",
      " 0.33481073 0.10207699 0.2095546  0.2838846  0.1526339  0.6822505\n",
      " 0.63857186 0.2692204  0.2740503  0.02527157 0.2095546  0.69252956\n",
      " 0.3507095  0.57756644 0.07603264 0.25840148 0.10023799 0.30127415\n",
      " 0.06946033 0.10875469 0.14488961 0.14197023 0.04425961 0.14279917\n",
      " 0.13628337 0.55687076 0.04749797 0.20003669 0.05047376 0.6477071\n",
      " 0.10648652 0.20169103 0.19299209 0.2685352  0.68946815 0.63738126\n",
      " 0.4820385  0.59279126 0.23457412 0.1635042  0.24403235 0.30127415\n",
      " 0.5370392  0.44882923 0.69222486 0.62748694 0.47505152 0.41586155\n",
      " 0.08710518 0.68912745 0.28105116 0.12624545 0.553609   0.0789165\n",
      " 0.35879898 0.1179124  0.5166848  0.62748694 0.63002086 0.2699068\n",
      " 0.26716834 0.6581017  0.6262054  0.04512142 0.68946815 0.21184485\n",
      " 0.2095546  0.13079514 0.4124239  0.26512718 0.59427595 0.12812407\n",
      " 0.08872446 0.08710518 0.68877184 0.34271204 0.21531661 0.63002086\n",
      " 0.5712986  0.6287588  0.4453393  0.11308175 0.68912745 0.08845268\n",
      " 0.47505152 0.40557343 0.57600844 0.45931327 0.04765036 0.53030425\n",
      " 0.5336786  0.46630877 0.275441   0.37192234 0.4470839  0.2095546\n",
      " 0.6798679  0.68912745 0.553609   0.07916132 0.67859197 0.10680792\n",
      " 0.6909472  0.06967787 0.518397   0.6751592  0.05732215 0.07144141\n",
      " 0.2095546  0.6590755  0.2370694  0.50981236 0.02789342 0.24339326\n",
      " 0.68167615 0.6409211  0.14958125 0.12147392 0.16679464 0.07626919\n",
      " 0.4942335  0.07301837 0.15981053 0.27128315 0.18875673 0.68586874\n",
      " 0.65510744 0.49944162 0.44882923 0.6909472  0.41586155 0.30127415\n",
      " 0.64987946 0.16774489 0.66812587 0.6249142  0.518397   0.4628108\n",
      " 0.50463533 0.4680577  0.21069731 0.2838846  0.61561465 0.20615558\n",
      " 0.37856194 0.6901047  0.25906855 0.30127415 0.19033617 0.25508404\n",
      " 0.55524224 0.3686216  0.12039561 0.4418527  0.5601133  0.44359553\n",
      " 0.3507095  0.3539345  0.68108755 0.6804848  0.5972234  0.19299209\n",
      " 0.40557343 0.5387141  0.5166848  0.43315405 0.65510744 0.30721605\n",
      " 0.50117457 0.5403853  0.64548945 0.33324233 0.38189995 0.30721605\n",
      " 0.5149702  0.28105116 0.6876161  0.5149702  0.5235174  0.6287588\n",
      " 0.43663013 0.28105116 0.59427595 0.59427595 0.21299724 0.6867718\n",
      " 0.28105116 0.5806639  0.22720589 0.4124239  0.29979986 0.58220315\n",
      " 0.52010643 0.52010643 0.11721128 0.18152604 0.1292629  0.6779331\n",
      " 0.56493914 0.15529245 0.23457412 0.633747   0.13748513 0.17110619\n",
      " 0.30497947 0.06544426 0.6911979  0.05448416 0.5420527  0.04397583\n",
      " 0.18980847 0.4262181  0.664658   0.60588264 0.06462678 0.30497947\n",
      " 0.11861726 0.10680792 0.17849714 0.43315405 0.11861726 0.23333386\n",
      " 0.05448416 0.09317746 0.0951888  0.31778398 0.1137609  0.16965887\n",
      " 0.16965887 0.69040054 0.2896074  0.19513825 0.5453756  0.44011074\n",
      " 0.69252956 0.39198303 0.5403853  0.36042753 0.69311666 0.0521098\n",
      " 0.54371613 0.6736901  0.1292629  0.5837359  0.24595705 0.30497947\n",
      " 0.10842801 0.69252956 0.09176452 0.07650645 0.12365607 0.04369375\n",
      " 0.05677971 0.04017958 0.15663639 0.10908221 0.36533386 0.6262054\n",
      " 0.16632122 0.47505152 0.4645598  0.0555326  0.26444918 0.39621297\n",
      " 0.13156721 0.04190168 0.43836987 0.30497947 0.13748513 0.3208422\n",
      " 0.2939481  0.56812996 0.59279126 0.3686216  0.23209848 0.27964154\n",
      " 0.30127415 0.6361803  0.53030425 0.6540854  0.5744444  0.18152604\n",
      " 0.68946815 0.18152604 0.6779331  0.431418   0.18152604 0.23581934\n",
      " 0.6844048  0.5957534  0.12774637 0.14531085 0.39875865 0.12183521\n",
      " 0.07121878 0.07121878 0.18356857 0.5370392  0.64548945 0.4680577\n",
      " 0.5149702  0.30423617 0.30423617 0.5453756  0.3458997  0.55524224\n",
      " 0.69252956 0.66978186 0.18356857 0.19086497 0.19513825 0.19513825\n",
      " 0.69252956 0.69252956 0.68840146 0.19513825 0.18875673 0.6114947\n",
      " 0.598686   0.39029557 0.39029557 0.6361803  0.21942233 0.07626919\n",
      " 0.55524224 0.3254612  0.25114322 0.4680577  0.23333386 0.10680792\n",
      " 0.06503431 0.4453393  0.42448777 0.23333386 0.67923695 0.02620416\n",
      " 0.39198303 0.55197114 0.47679916 0.21942233 0.15219462 0.15219462\n",
      " 0.15219462 0.53199303 0.13588488 0.05095944 0.05112229 0.11073306\n",
      " 0.10680792 0.69277346 0.18000641]\n"
     ]
    }
   ],
   "source": [
    "# Predictive entropy on youtube comments test data\n",
    "ent = entropy_numpy(stu_probs)\n",
    "print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5be0ef2a-f710-4df9-9a13-d2becb9f5558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38787\n"
     ]
    }
   ],
   "source": [
    "print(ent.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
