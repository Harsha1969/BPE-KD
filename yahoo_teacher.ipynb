{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0db98e8-a64e-44a4-97f5-9caa9350afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971af11a-a4d3-418a-806f-c2f212cafd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from bpe import BayesPE \n",
    "from llm_model import LLM\n",
    "import evaluation\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc041bf9-c205-4cb4-9fd3-0d95df6413a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task instructions\n",
    "instructions = [\n",
    "    'classify the question and answer below into one of the following topics:',\n",
    "    'Assign a topic label to the following question and answer from the list provided:',\n",
    "    'Determine which topic best fits the question and answer shown below:',\n",
    "    'Categorize the following Q&A under one of these topics:',\n",
    "    'Select the most appropriate topic for the question and answer pair below:',\n",
    "    'Choose the correct topic category for the given question and answer:',\n",
    "    'Identify the topic that the following question and answer belong to:',\n",
    "    'Match the question and answer below to the relevant topic:',\n",
    "    'Label the question and answer below with the most fitting topic from the list:'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77eedd6e-03a3-4e4e-8b67-86d1a57128ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Yahoo Answers train and test data\n",
    "df_train = pd.read_csv('train_yahoo.csv', header=None)\n",
    "df_test = pd.read_csv('test_yahoo.csv', header=None)\n",
    "\n",
    "n_train = 50000  \n",
    "n_in_context = 5  \n",
    "n_val = 100\n",
    "n_test = 5000\n",
    "n_total_in_context = len(instructions) * n_in_context\n",
    "df_train_actual = df_train.iloc[:n_train]\n",
    "df_in_context_base = df_train.iloc[n_train:n_train + n_total_in_context]\n",
    "df_val = df_train.iloc[n_train + n_total_in_context:n_train + n_total_in_context + n_val]\n",
    "df_test_actual = df_test.iloc[:n_test]\n",
    "def format_prompt(q1, q2, a):\n",
    "    return \"Question: \" + q1.astype(str) + \" \" + q2.astype(str) + \"\\nAnswer: \" + a.astype(str)\n",
    "\n",
    "gt_labels_train = df_train_actual.iloc[:, 0].values.astype(int)\n",
    "samples_train = format_prompt(df_train_actual.iloc[:, 1], df_train_actual.iloc[:, 2], df_train_actual.iloc[:, 3]).values\n",
    "gt_labels_val = df_val.iloc[:, 0].values.astype(int)\n",
    "samples_val = format_prompt(df_val.iloc[:, 1], df_val.iloc[:, 2], df_val.iloc[:, 3]).values\n",
    "gt_labels_test = df_test_actual.iloc[:, 0].values.astype(int)\n",
    "samples_test = format_prompt(df_test_actual.iloc[:, 1], df_test_actual.iloc[:, 2], df_test_actual.iloc[:, 3]).values\n",
    "\n",
    "for i in range(len(instructions)):\n",
    "    start_idx = i * n_in_context\n",
    "    end_idx = (i + 1) * n_in_context\n",
    "    df_in_context = df_in_context_base.iloc[start_idx:end_idx]\n",
    "\n",
    "    samples_in_context_i = format_prompt(df_in_context.iloc[:, 1], df_in_context.iloc[:, 2], df_in_context.iloc[:, 3]).values\n",
    "    gt_labels_in_context_i = df_in_context.iloc[:, 0].values.astype(int)\n",
    "\n",
    "    if i == 0:\n",
    "        samples_in_context = np.expand_dims(samples_in_context_i, axis=1)\n",
    "        gt_labels_in_context = np.expand_dims(gt_labels_in_context_i, axis=1)\n",
    "    else:\n",
    "        samples_in_context = np.concatenate((samples_in_context, np.expand_dims(samples_in_context_i, axis=1)), axis=1)\n",
    "        gt_labels_in_context = np.concatenate((gt_labels_in_context, np.expand_dims(gt_labels_in_context_i, axis=1)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58acd5c0-369e-493a-9566-e10b291755a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b984ab5c224d72a7755cc22d7a1efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 1:\n",
      "classify the question and answer below into one of the following topics:\n",
      "1. Society & Culture\n",
      "2. Science & Mathematics\n",
      "3. Health\n",
      "4. Education & Reference\n",
      "5. Computers & Internet\n",
      "6. Sports\n",
      "7. Business & Finance\n",
      "8. Entertainment & Music\n",
      "9. Family & Relationships\n",
      "10. Politics & Government\n",
      "\n",
      "Question: when you talk about the volume of a gas are you refering to the volume of the molecules themselves? explain?\n",
      "Answer: No, the volume refers to the total space in which those molecules are found moving around (should be the same as the volume of the container). In any case, atoms and molecules are pretty much all empty space themselves - most of the mass is concentrated in the nucleus, but the electron cloud takes up a lot more space.\n",
      "the topic is Science & Mathematics\n",
      "\n",
      "EXAMPLE 2:\n",
      "classify the question and answer below into one of the following topics:\n",
      "1. Society & Culture\n",
      "2. Science & Mathematics\n",
      "3. Health\n",
      "4. Education & Reference\n",
      "5. Computers & Internet\n",
      "6. Sports\n",
      "7. Business & Finance\n",
      "8. Entertainment & Music\n",
      "9. Family & Relationships\n",
      "10. Politics & Government\n",
      "\n",
      "Question: benefits of combining financial and process auditing in one department? we have departments (financial auditing and operation auditing) and we want to combine them into one internal auditing department.\n",
      "Answer: You would save on overhead costs from having to pay for equipment, utilties, and upkeep on two separate offices.\n",
      "the topic is Business & Finance\n",
      "\n",
      "EXAMPLE 3:\n",
      "classify the question and answer below into one of the following topics:\n",
      "1. Society & Culture\n",
      "2. Science & Mathematics\n",
      "3. Health\n",
      "4. Education & Reference\n",
      "5. Computers & Internet\n",
      "6. Sports\n",
      "7. Business & Finance\n",
      "8. Entertainment & Music\n",
      "9. Family & Relationships\n",
      "10. Politics & Government\n",
      "\n",
      "Question: What Ivy League universities did the lovers of Love Story attended?  \n",
      "Answer: He studied law at Harvard; she studied music at Radcliffe\n",
      "the topic is Entertainment & Music\n",
      "\n",
      "EXAMPLE 4:\n",
      "classify the question and answer below into one of the following topics:\n",
      "1. Society & Culture\n",
      "2. Science & Mathematics\n",
      "3. Health\n",
      "4. Education & Reference\n",
      "5. Computers & Internet\n",
      "6. Sports\n",
      "7. Business & Finance\n",
      "8. Entertainment & Music\n",
      "9. Family & Relationships\n",
      "10. Politics & Government\n",
      "\n",
      "Question: Ancient Olympics help? I need help with the ancient olympics, Please Help.\\n\\nWhy were they held?\\nWhat was in them, What games did they play?\\n\\nI will rate the best answer to  whoever answers those two questions.\n",
      "Answer: The Ancient Olympics were held in Greece up to the 1st century BC when the Romans banned them. They were held to celebrate Zeus and the Greek Gods and during the Games, an Olympic truce was established. As to what was in those Games, everything from running, to wrestling, to chariot racing\n",
      "the topic is Business & Finance\n",
      "\n",
      "EXAMPLE 5:\n",
      "classify the question and answer below into one of the following topics:\n",
      "1. Society & Culture\n",
      "2. Science & Mathematics\n",
      "3. Health\n",
      "4. Education & Reference\n",
      "5. Computers & Internet\n",
      "6. Sports\n",
      "7. Business & Finance\n",
      "8. Entertainment & Music\n",
      "9. Family & Relationships\n",
      "10. Politics & Government\n",
      "\n",
      "Question: my search results say 1-1000 out of 80,000.. how do I get beyond the 1000, to the 80,000 ??  \n",
      "Answer: True, Google says:\\n\\n   Sorry, Google does not serve more than 1000 results for any query.\\n\\nBut here is a fast way to get way down in the list.\\n\\nFor example, go to Google and search for \"hecate\".\\n\\nScroll to the bottom of the screen and click on number 2\\n\\nNow the web address in your browser address bar will read:\\n\\nhttp: // www.google.com/ search?q=hecate &hl=en&lr=&safe=off &start=10 &sa=N\\n\\nsimply type \"9\" in front of the \"10\" in the part of the URL which reads \"start=10\"\\n\\nNow it will read \"start=910\", press Enter and ZOOM you are way  down the list!\\n\\n6sj7\n",
      "the topic is Business & Finance\n",
      "\n",
      "EXAMPLE 6:\n",
      "classify the question and answer below into one of the following topics:\n",
      "1. Society & Culture\n",
      "2. Science & Mathematics\n",
      "3. Health\n",
      "4. Education & Reference\n",
      "5. Computers & Internet\n",
      "6. Sports\n",
      "7. Business & Finance\n",
      "8. Entertainment & Music\n",
      "9. Family & Relationships\n",
      "10. Politics & Government\n",
      "\n",
      "<SAMPLE_IN>\n",
      "the topic is <LABEL_OUT>\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt formatting class for topic classification and initializes an LLM-based classifier\n",
    "class PromptFormatting(object):\n",
    "    def __init__(self):\n",
    "        self.INSTRUCTION = 'classify the question and answer below into one of the following topics:'\n",
    "        self.CLASSES = [\n",
    "    'Society & Culture',\n",
    "    'Science & Mathematics',\n",
    "    'Health',\n",
    "    'Education & Reference',\n",
    "    'Computers & Internet',\n",
    "    'Sports',\n",
    "    'Business & Finance',\n",
    "    'Entertainment & Music',\n",
    "    'Family & Relationships',\n",
    "    'Politics & Government'\n",
    "]\n",
    "        self.CLASSES_FOR_MATCHING = [self.CLASSES]\n",
    "        self.CLASSES_TEXT = '''1. {}\\n2. {}\\n3. {}\\n4. {}\\n5. {}\\n6. {}\\n7. {}\\n8. {}\\n9. {}\\n10. {}'''.format(self.CLASSES[0],self.CLASSES[1], self.CLASSES[2], self.CLASSES[3], self.CLASSES[4], self.CLASSES[5], self.CLASSES[6], self.CLASSES[7], self.CLASSES[8], self.CLASSES[9])\n",
    "    def format_instruction(self, instruction):\n",
    "        return '''{}\\n{}\\n'''.format(instruction, self.CLASSES_TEXT)\n",
    "\n",
    "    def format_content(self, content):\n",
    "        return '''{}\\nthe topic is '''.format(content)\n",
    "\n",
    "prompt_formatting = PromptFormatting()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize BayesPE (Teacher Model)\n",
    "bayespe_classifier = BayesPE(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "    prompt_formatting=prompt_formatting,\n",
    "    instructions=instructions, \n",
    "    few_shot_texts_sets=samples_in_context, \n",
    "    few_shot_labels_sets=gt_labels_in_context, \n",
    "    use_reduced_precision=True\n",
    ")\n",
    "\n",
    "# Print example prompt\n",
    "bayespe_classifier.print_prompt_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d6346d5-3c3e-4806-8cdc-1b674ab045e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 1 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:21<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 2 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:21<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 3 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:20<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 4 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:23<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 5 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:18<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 6 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:19<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 7 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:20<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 8 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:23<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 9 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:26<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss: 157.72197375504285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.20779699, 0.12849686, 0.06699287, 0.07335307, 0.0165612 ,\n",
       "       0.0174329 , 0.3899038 , 0.0432402 , 0.05622215], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize prompt weights\n",
    "bayespe_classifier.optimise_weights(samples_val, gt_labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee9bc5e-e630-459d-bd56-4972298c7304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 1 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [2:34:49<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 2 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [2:47:11<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 3 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [3:16:54<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 4 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [3:41:26<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 5 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [2:51:42<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 6 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [3:14:01<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 7 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [3:02:30<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 8 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [2:58:27<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 9 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [2:54:00<00:00,  4.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get prompt weights and prompt wise probabilities on train data\n",
    "_,probs,weights = bayespe_classifier.forward(samples_train, n_forward_passes=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49585aad-7965-4f20-9c81-18700052f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(probs,'yahoo_probs.pt')\n",
    "torch.save(weights,'yahoo_prompt_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb2604da-1af2-4e56-b87d-c1805ccf0186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 1 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [15:23<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 2 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [16:37<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 3 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [19:32<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 4 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [22:08<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 5 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [17:07<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 6 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [19:24<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 7 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [18:11<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 8 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [17:44<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference for promt 9 out of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [17:19<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 9808.3280 seconds\n",
      "[[1.93036354e-01 2.57357897e-05 3.65092082e-05 5.45402775e-03\n",
      "  1.26509534e-05 2.28543786e-05 2.70930552e-05 2.19088936e-03\n",
      "  7.99151768e-01 4.21669012e-05]\n",
      " [1.35133549e-01 8.50703641e-01 2.55414551e-03 1.08454302e-02\n",
      "  3.69958667e-05 8.94036967e-05 1.82077871e-05 3.35173085e-04\n",
      "  2.58672280e-04 2.48294007e-05]\n",
      " [4.56086223e-02 5.87470741e-02 2.42778728e-04 9.58485288e-02\n",
      "  4.28586243e-04 3.46439326e-03 7.02500764e-05 7.87274751e-01\n",
      "  8.14672994e-03 1.68333976e-04]\n",
      " [1.41373868e-05 2.61183168e-05 1.04747918e-05 9.99873712e-01\n",
      "  1.19752805e-05 1.01062609e-05 1.32049412e-05 1.46576673e-05\n",
      "  1.51186073e-05 1.05436736e-05]\n",
      " [4.40773411e-04 1.32909309e-02 9.80813199e-01 4.28599835e-03\n",
      "  1.04046251e-05 1.14920683e-05 1.05185334e-05 1.28341023e-05\n",
      "  1.10336284e-03 2.05349281e-05]\n",
      " [4.77729435e-01 1.17814362e-04 3.41906512e-02 8.39525247e-03\n",
      "  4.76183763e-05 4.25647387e-05 1.95559361e-02 3.28599674e-04\n",
      "  4.46479828e-01 1.31123485e-02]\n",
      " [2.05624900e-04 7.34238616e-05 1.25852883e-05 3.23927278e-03\n",
      "  6.12111180e-01 5.58635436e-05 4.46136658e-04 3.83811198e-01\n",
      "  3.10250308e-05 1.37379775e-05]\n",
      " [4.39327933e-03 9.32319336e-03 8.49302235e-01 1.28797971e-01\n",
      "  1.68165967e-04 2.20428856e-05 5.62498550e-03 1.03368803e-03\n",
      "  8.02449061e-04 5.32037562e-04]\n",
      " [2.26591126e-02 3.78968869e-01 4.57242397e-04 4.05780001e-02\n",
      "  2.99197936e-04 5.58367705e-03 7.48086997e-05 5.42376599e-01\n",
      "  8.93857470e-03 6.39661235e-05]\n",
      " [4.37445539e-03 6.23318754e-03 1.46946776e-05 2.86972540e-01\n",
      "  7.31017571e-05 6.96874933e-05 5.35302183e-05 7.01944358e-01\n",
      "  2.47850375e-04 1.66424661e-05]]\n",
      "Teacher f1-score: 0.6077004613777541, Teacher ECE: 0.19333063066005707\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BayesPE performance on yahoo answers test data\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "teacher_probs,_,_ = bayespe_classifier.forward(samples_test, n_forward_passes=9)\n",
    "end.record()\n",
    "\n",
    "# Wait for the events to be recorded\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Report in seconds\n",
    "elapsed_time_ms = start.elapsed_time(end) \n",
    "elapsed_time_sec = elapsed_time_ms / 1000  \n",
    "\n",
    "print(f\"Inference time: {elapsed_time_sec:.4f} seconds\")\n",
    "print(teacher_probs[:10, :])\n",
    "f1_score = evaluation.compute_metric(gt_labels_test, teacher_probs, metric='f1')\n",
    "ece = evaluation.compute_metric(gt_labels_test, teacher_probs, metric='ece')\n",
    "print('Teacher f1-score: {}, Teacher ECE: {}'.format(f1_score, ece))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
